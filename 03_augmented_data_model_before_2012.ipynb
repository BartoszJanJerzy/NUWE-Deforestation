{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from process.utils import load_history_graph\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = {\n",
    "    'data': os.path.join(os.getcwd(), 'data'),\n",
    "    'models': os.path.join(os.getcwd(), 'models'),\n",
    "    'train': os.path.join(os.getcwd(), 'data', 'train'),\n",
    "    'test': os.path.join(os.getcwd(), 'data', 'test')\n",
    "}\n",
    "\n",
    "def load_datafile_path(file: str) -> str: return os.path.join(PATHS['data'], file)\n",
    "def load_modelfile_path(file: str) -> str: return os.path.join(PATHS['models'], file)\n",
    "def load_train_image_path(file: str) -> str: return os.path.join(PATHS['train'], file)\n",
    "def load_test_image_path(file: str) -> str: return os.path.join(PATHS['test'], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = pd.read_feather(load_datafile_path('train.ftr'))\n",
    "train_info = train_info[train_info['year'] < 2012]\n",
    "train_info = (\n",
    "    train_info\n",
    "    .sample(len(train_info))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "labels = train_info['label'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_paths = train_info['example_path']\n",
    "images_names = [p.split('/')[-1] for p in images_paths]\n",
    "images = [cv2.imread(load_train_image_path(images_names[i])) for i in tqdm(range(len(images_names)))]\n",
    "\n",
    "for img in images:\n",
    "    assert img.shape == (332, 332, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(load_datafile_path('augmented_data'), 'rb') as file:\n",
    "    data_aug = pickle.load(file)\n",
    "\n",
    "for img in data_aug['images']:\n",
    "    assert img.shape == (332, 332, 3)\n",
    "\n",
    "for i in range(len(data_aug['images'])):\n",
    "    images.append(data_aug['images'][i])\n",
    "    labels.append(data_aug['labels'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = train_info['label'].nunique()\n",
    "img_height = images[0].shape[0]\n",
    "img_width = images[0].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = len(images)\n",
    "val_count = 500\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 2:\n",
    "        labels[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.array(images[:total_count-val_count])\n",
    "val_images = np.array(images[total_count-val_count:])\n",
    "assert len(train_images) + len(val_images) == total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(labels[:total_count-val_count])\n",
    "val_labels = np.array(labels[total_count-val_count:])\n",
    "assert len(train_labels) + len(val_labels) == total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(8, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(32, activation='relu'),\n",
    "  layers.Dense(class_num)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.1,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  validation_data=(val_images, val_labels),\n",
    "  epochs=epochs,\n",
    "  batch_size=batch_size,\n",
    "  callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "assert len(acc) == len(val_acc) == len(loss) == len(val_loss)\n",
    "\n",
    "fig = load_history_graph(acc, val_acc, loss, val_loss)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(load_modelfile_path('augmented_3k'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df334b9baf988a808dd302c0f32b1b4e4d2454b9dbc8352ea80b8ff7878e3aeb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
